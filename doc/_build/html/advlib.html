

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>AdversariaLib’s documentation &mdash; AdversariaLib 1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="AdversariaLib 1.0 documentation" href="index.html" />
    <link rel="next" title="Matlab Wrappers for AdversariaLib (with Examples)" href="matlab.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="matlab.html" title="Matlab Wrappers for AdversariaLib (with Examples)"
             accesskey="N">next</a> |</li>
        <li><a href="#">AdversariaLib 1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <img alt="_images/advlib_logo.png" src="_images/advlib_logo.png" />
<div class="section" id="adversarialib-s-documentation">
<h1>AdversariaLib&#8217;s documentation<a class="headerlink" href="#adversarialib-s-documentation" title="Permalink to this headline">¶</a></h1>
<dl class="docutils">
<dt>Contents:</dt>
<dd><ul class="first last simple">
<li><a class="reference internal" href="#adversarial-machine-learning">Adversarial Machine Learning</a></li>
<li><a class="reference internal" href="#about-adversarialib">About AdversariaLib</a></li>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#usage-examples">Usage Examples</a></li>
<li><a class="reference internal" href="#developers-section">Developers Section</a></li>
<li><a class="reference internal" href="#getting-help-and-sending-suggestions">Getting Help and Sending Suggestions</a></li>
<li><a class="reference internal" href="#contribute">Contribute</a></li>
<li><a class="reference internal" href="#snapshots">Snapshots</a></li>
</ul>
</dd>
</dl>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="matlab.html">Matlab Wrappers for AdversariaLib (with Examples)</a><ul class="simple">
</ul>
</li>
</ul>
</div>
<div class="section" id="adversarial-machine-learning">
<span id="id1"></span><h2>Adversarial Machine Learning<a class="headerlink" href="#adversarial-machine-learning" title="Permalink to this headline">¶</a></h2>
<p>Machine-learning and pattern-recognition techniques such as Support Vector Machines (SVMs), Hidden Markov Models (HMMs), N-grams, Decision Trees, Artificial Neural Networks (ANN) are increasingly being adopted in security applications like spam filtering, network intrusion detection, and malware detection, due to their ability to generalize, and to potentially detect novel attacks or variants of known ones.</p>
<p>However, learning algorithms typically assume data <em>stationarity</em>: that is, both the data used to train the classifier and the operational data it classifies are sampled from the same (though possibly unknown) distribution. Meanwhile, in adversarial settings such as the above mentioned ones, intelligent and adaptive adversaries may purposely manipulate data (violating stationarity) to exploit existing vulnerabilities of learning algorithms, and to impair the entire system.</p>
<p>This has led to an arms race between the designers of learning systems and their adversaries. <em>Classical performance measures cannot be directly exploited to assess the security of machine learning algorithms and to measure the performance degradation they may incur under carefully targeted attacks.</em></p>
<p>This raises several open issues, related to whether machine-learning techniques can be safely adopted in security-sensitive tasks, or if they must (and can) be re-designed for this purpose. In particular, the main open issues to be addressed include:</p>
<ul class="simple">
<li><strong>analyzing the vulnerabilities of learning algorithms;</strong></li>
<li><strong>evaluating their security by implementing the corresponding attacks;</strong></li>
<li><strong>designing suitable countermeasures</strong>.</li>
</ul>
<p>These issues are currently explored in the emerging research area of <em>adversarial machine learning</em>, at the intersection between computer security and machine learning.</p>
</div>
<div class="section" id="about-adversarialib">
<span id="id2"></span><h2>About AdversariaLib<a class="headerlink" href="#about-adversarialib" title="Permalink to this headline">¶</a></h2>
<p><tt class="docutils literal"><span class="pre">AdversariaLib</span></tt> is a general-purpose library for the automatic evaluation of machine learning-based classifiers under adversarial attacks. It comes with a set of powerful features:</p>
<ul class="simple">
<li><strong>Easy-to-use</strong> Running sophisticated experiments is as easy as launch a single script. Experimental settings can be defined through a single setup file.</li>
<li><strong>Wide range of supported ML algorithms</strong> All supervised learning algorithms supported by <a class="reference external" href="http://scikit-learn.org/stable/supervised_learning.html">scikit-learn</a> (yes, they are a lot!), as well as Artificial Neural Networks (ANNs), by means of our scikit-learn wrapper for <a class="reference external" href="http://leenissen.dk/fann/wp/">FANN</a>. In the current implementation, the library allows the security evaluation of SVMs having kernel either linear, or rbf, or polynomial, and ANNs having one hidden layer.</li>
<li><strong>Fast Learning and Evaluation</strong> Thanks to <a class="reference external" href="http://scikit-learn.org/stable/supervised_learning.html">scikit-learn</a> and <a class="reference external" href="http://leenissen.dk/fann/wp/">FANN</a>, all supported ML algorithms are optimized and written in C/C++ language. Hopefully, this allows you to meet a conference deadline ;-)</li>
<li><strong>Built-in attack algorithms</strong> Gradient Descent Attack</li>
<li><strong>Extensible</strong> Other attack algorithms can be easily added to the library.</li>
<li><strong>Multi-processing</strong> Do you want to further save time? The built-in attack algorithms can run concurrently on multiple processors.</li>
</ul>
<p>Last, but not least, <tt class="docutils literal"><span class="pre">AdversariaLib</span></tt> is <strong>free software</strong>, released under the <a class="reference external" href="http://www.gnu.org/licenses/gpl.txt">GNU General Public License version 3</a>!</p>
<p><tt class="docutils literal"><span class="pre">Authors:</span></tt> <a class="reference external" href="http://pralab.diee.unica.it/en/IginoCorona">Igino Corona</a>, <a class="reference external" href="http://pralab.diee.unica.it/en/BattistaBiggio">Battista Biggio</a>, <a class="reference external" href="http://pralab.diee.unica.it/en/DavideMaiorca">Davide Maiorca</a>, Pattern Recognition and Applications Lab, Dept. of Electrical and Electronic Engineering, University of Cagliari, Italy.</p>
</div>
<div class="section" id="installation">
<span id="id3"></span><h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p><tt class="docutils literal"><span class="pre">AdversariaLib</span></tt> is <em>multi-platform</em> software, built upon the <a class="reference external" href="http://scikit-learn.org/stable/supervised_learning.html">scikit-learn</a> framework. We tested it on <a class="reference external" href="http://releases.ubuntu.com/raring/">Ubuntu 13.04</a> and <a class="reference external" href="http://www.apple.com/it/osx/">Apple Mac OS X 10.8.4</a>, but it can certainly run on other operating systems, such as Microsoft Windows.</p>
<ol class="arabic">
<li><dl class="first docutils">
<dt>Install all required packages: <tt class="docutils literal"><span class="pre">scikit-learn</span></tt> and <tt class="docutils literal"><span class="pre">pyfann</span></tt></dt>
<dd><p class="first">On <strong>Ubuntu</strong> (from command shell):</p>
<div class="highlight-python"><pre>$ sudo apt-get install python-sklearn python-pyfann python-pip
$ sudo pip install scikit-learn --upgrade</pre>
</div>
<dl class="docutils">
<dt>On <strong>Mac OS X</strong>:</dt>
<dd><ol class="first last arabic">
<li><p class="first">Install the latest version of <a class="reference external" href="https://developer.apple.com/xcode/">Xcode</a> from the AppStore</p>
</li>
<li><dl class="first docutils">
<dt>Install the latest version of <a class="reference external" href="https://www.macports.org/install.php">macports</a>. If <a class="reference external" href="https://www.macports.org/install.php">macports</a> have been already installed please update them through the following shell commands:</dt>
<dd><div class="first last highlight-python"><pre>$ sudo port selfupdate
$ sudo port upgrade outdated</pre>
</div>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Run the following instructions from command shell:</dt>
<dd><div class="first last highlight-python"><pre>$ sudo port install py-scikit-learn py-setuptools</pre>
</div>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Setup the python interpreter installed by macports (in our case, <tt class="docutils literal"><span class="pre">python27</span></tt>) as default:</dt>
<dd><div class="first highlight-python"><pre>$ port select --list python
Available versions for python:
        none (active)
        python25-apple
        python26-apple
        python27
        python27-apple
$ sudo port select --set python python27
$ sudo mv /usr/bin/python /usr/bin/python.orig
$ sudo ln -s /opt/local/bin/python /usr/bin/python</pre>
</div>
<dl class="last docutils">
<dt><strong>NOTE</strong>: we are assuming that the default python interpreter corresponds to <tt class="docutils literal"><span class="pre">/usr/bin/python</span></tt> (run the <tt class="docutils literal"><span class="pre">which</span> <span class="pre">python</span></tt> instruction). You may <strong>restore</strong> your default python interpreter <em>anytime</em>, through the instructions:</dt>
<dd><div class="first last highlight-python"><pre>$ sudo mv /usr/bin/python.orig /usr/bin/python</pre>
</div>
</dd>
</dl>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>We suggest to install our patched version of Python bindings for FANN within the AdversariaLib repository (<tt class="docutils literal"><span class="pre">wrappers/fann-2.1.0</span></tt> folder), because the original one contains a bug in <tt class="docutils literal"><span class="pre">line</span> <span class="pre">111</span></tt> of file <tt class="docutils literal"><span class="pre">pyfann.i</span></tt>: check out the following <a class="reference external" href="http://leenissen.dk/fann/forum/viewtopic.php?f=17&amp;t=588">fann post</a>. On UNIX-like OSs such as Mac OS X (open a shell inside <tt class="docutils literal"><span class="pre">wrappers/fann-2.1.0</span></tt>):</dt>
<dd><div class="first highlight-python"><pre>$ chmod +x configure
$ ./configure
$ make
$ cd python
$ python setup.py install --user</pre>
</div>
<dl class="last docutils">
<dt><strong>NOTE</strong>: the pyfann library is now accessible to the current user only. If you want a system-wide installation, please <strong>substitute</strong> the latter instruction with:</dt>
<dd><div class="first last highlight-python"><pre>$ sudo python setup.py install</pre>
</div>
</dd>
</dl>
</dd>
</dl>
</li>
</ol>
</dd>
<dt>For <strong>other OSs</strong>, please refer to:</dt>
<dd><ul class="first last simple">
<li><a class="reference external" href="http://scikit-learn.org/stable/install.html">installation section of scikit-learn</a></li>
<li>See previous point (5) and the <a class="reference external" href="http://leenissen.dk/fann/html/files2/installation-txt.html">installation section of FANN</a></li>
</ul>
</dd>
</dl>
<p class="last">Please note that <tt class="docutils literal"><span class="pre">AdversariaLib</span></tt> has been tested with scikit-learn version <em>0.13.1</em> and pyfann version <em>2.1.0</em>.</p>
</dd>
</dl>
</li>
<li><p class="first">Download <tt class="docutils literal"><span class="pre">AdversariaLib</span></tt> from the <a class="reference external" href="https://sourceforge.net/projects/adversarialib/files/adversarialib_v1.0/">official repository</a> and unpack it on a folder of your choice.</p>
</li>
</ol>
</div>
<div class="section" id="usage-examples">
<span id="id4"></span><h2>Usage Examples<a class="headerlink" href="#usage-examples" title="Permalink to this headline">¶</a></h2>
<p>In order to provide a <em>quick-and-dirty</em> way for exploiting our adversariaLib and demonstrating its features, we prepared a set of sample <cite>experiments</cite>. For instance, performing a <em>Gradient Descent Attack</em> <a class="reference internal" href="#ecml2013">[ECML2013]</a> against a <em>Linear Support Vector Machine</em> (Linear SVM), with full and partial knowledge of its training data is as easy as running the following instruction (from a shell within the main folder of the project):</p>
<div class="highlight-python"><pre>$ python adversariaLib/runexp.py experiments/svm_lin_attack</pre>
</div>
<p>Similarly, it is easy to perform the same attack simulation against a <em>SVM with RBF kernel</em>:</p>
<div class="highlight-python"><pre>$ python adversariaLib/runexp.py experiments/svm_rbf_attack</pre>
</div>
<p>...or against a <em>Multi-Layer Perceptron</em> (MLP), also known as Artificial Neural Network:</p>
<div class="highlight-python"><pre>$ python adversariaLib/runexp.py experiments/mlp_attack</pre>
</div>
<p>We also demostrate how it is easy to simulate an attack against <em>MLP</em>, <em>Linear SVM</em> and <em>SVM with RBF kernel</em>, when the adversary does not know exactly the learning algorithm employed by the targeted classifier and thus <cite>assumes</cite> a <em>Linear SVM</em>:</p>
<div class="highlight-python"><pre>$ python adversariaLib/runexp.py experiments/svm_lin_attack_vs_multiple</pre>
</div>
<div class="section" id="general-notes">
<h3>General Notes<a class="headerlink" href="#general-notes" title="Permalink to this headline">¶</a></h3>
<p>When an experiment is launched three subfolders are automatically created within the main folder of the experiment, namely, <tt class="docutils literal"><span class="pre">classifiers</span></tt>, <tt class="docutils literal"><span class="pre">datasets</span></tt>, <tt class="docutils literal"><span class="pre">tests</span></tt>. <tt class="docutils literal"><span class="pre">adversariaLib</span></tt> further subdivides experimental data into two types: <tt class="docutils literal"><span class="pre">BASE</span></tt> and <tt class="docutils literal"><span class="pre">ATTACK</span></tt>. <tt class="docutils literal"><span class="pre">BASE</span></tt> data is related to all standard procedures for learning and testing, while <tt class="docutils literal"><span class="pre">ATTACK</span></tt> data is related to attacks against classifiers/machine-learning algorithms.  This is the reason why each one of the three folders <tt class="docutils literal"><span class="pre">classifiers</span></tt>, <tt class="docutils literal"><span class="pre">datasets</span></tt>, <tt class="docutils literal"><span class="pre">tests</span></tt> contains two subfolders, namely, <tt class="docutils literal"><span class="pre">base</span></tt> (<tt class="docutils literal"><span class="pre">exp_type=BASE</span></tt>) and <tt class="docutils literal"><span class="pre">attack</span></tt> (<tt class="docutils literal"><span class="pre">exp_type=ATTACK</span></tt>). The attack folder is further populated with a subfolder having the name of (and data related to) the related attack (e.g., <tt class="docutils literal"><span class="pre">gradient_descent</span></tt>).</p>
<div class="section" id="folders-and-file-format">
<h4>Folders and File Format<a class="headerlink" href="#folders-and-file-format" title="Permalink to this headline">¶</a></h4>
<p>The output of an experiment consists of the standard (shell) output, containing real-time information about runtime operations, as well as <strong>a number of files containing different kind of information</strong>. In general, the folder:</p>
<ul>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">datasets/base</span></tt> contains:</dt>
<dd><ul class="first last simple">
<li>training (<tt class="docutils literal"><span class="pre">tr</span></tt>) and testing (<tt class="docutils literal"><span class="pre">ts</span></tt>) splits; filename format: <tt class="docutils literal"><span class="pre">&lt;dataset_name&gt;.&lt;split_number&gt;.&lt;tr|ts&gt;.txt</span></tt>. Each line contains the index of the pattern associated to the dataset split. Each training/testing split is randomly sampled from the original dataset.</li>
<li>the indexes of patterns related to each cross-validation fold: <tt class="docutils literal"><span class="pre">&lt;dataset_name&gt;.kfold</span></tt>. This file contains cross-validation folds. We use the same for all training splits.</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">classifiers/base</span></tt> contains all targeted classifiers; filename format: <tt class="docutils literal"><span class="pre">&lt;targeted_classifier_type&gt;.&lt;dataset_name&gt;.&lt;split_number&gt;.tr.cdat</span></tt>. <tt class="docutils literal"><span class="pre">&lt;targeted_classifier_type&gt;</span></tt> is a keyword specified in the setup file, that identifies: training algorithm (e.g. linear SVM), as well as training settings (see the <tt class="docutils literal"><span class="pre">CLASSIFIER_PARAMS</span></tt> variable, described in <a class="reference internal" href="#experimental-parameters">Experimental Parameters</a>).</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">tests/base</span></tt> contains all test scores/labels; filename format: <tt class="docutils literal"><span class="pre">&lt;targeted_classifier_type&gt;.&lt;dataset_name&gt;.&lt;split_number&gt;.ts.txt</span></tt>. Each line contains (space-separated) score and label assigned by <tt class="docutils literal"><span class="pre">&lt;targeted_classifier_type&gt;.&lt;dataset_name&gt;.&lt;split_number&gt;.tr.cdat</span></tt> to the corresponding pattern in <tt class="docutils literal"><span class="pre">&lt;dataset_name&gt;.&lt;split_number&gt;.ts.txt</span></tt>.</p>
</li>
</ul>
<p>In addition, other files will be generated, according to the operations performed by each attack. See the following section for information stored by the <em>Gradient Descent Attack</em>.</p>
</div>
<div class="section" id="surrogate-classifiers">
<h4>Surrogate Classifiers<a class="headerlink" href="#surrogate-classifiers" title="Permalink to this headline">¶</a></h4>
<p>When an attack takes place, one or more <em>surrogate</em> classifiers are built for emulating the targeted classifier.
Let us define as <em>surrogate classifier</em> the classifier whose parameters are known to the adversary.
As a special case, when the attacker has got a perfect knowledge (i.e., defender&#8217;s worst case), the surrogate classifier corresponds to the targeted classifier.
For each testing split, one or more <em>surrogate</em> classifiers are trained on a random subset of it.
In order to evaluate the average performance of an attack based on surrogate classifiers given a <strong>fixed number of training samples</strong>,
it is possible to create multiple instances of them, sampling multiple times the same testing split.</p>
</div>
</div>
<div class="section" id="gradient-descent-attack-notes">
<span id="id6"></span><h3>Gradient Descent Attack Notes<a class="headerlink" href="#gradient-descent-attack-notes" title="Permalink to this headline">¶</a></h3>
<p>Each main iteration of the attack corresponds to a different <tt class="docutils literal"><span class="pre">boundary_number</span></tt>, i.e., a different constraint.
The higher the <tt class="docutils literal"><span class="pre">boundary_number</span></tt>, the less the constraint on the distance between a computed attack pattern and the root (initial)
attack pattern given to the gradient-descent algorithm. See the <tt class="docutils literal"><span class="pre">max_boundaries</span></tt> option, described in <a class="reference internal" href="#experimental-parameters">Experimental Parameters</a>.</p>
<div class="section" id="kde-distance">
<span id="id7"></span><h4>KDE distance<a class="headerlink" href="#kde-distance" title="Permalink to this headline">¶</a></h4>
<p>The KDE distance can be exploited by the mimicry component of the attack. We calculate the distance between an attack pattern and the distribution of a subset <img class="math" src="_images/math/859ccf4cd60c7bc6b8fa1afc9a42dc811a826d6f.png" alt="L"/> of legitimate patterns, estimated through <a class="reference external" href="http://en.wikipedia.org/wiki/Kernel_density_estimation">kernel density estimation</a>. In particular, we consider only the <img class="math" src="_images/math/e60cd475d856cbc36f4890d5d86f966c41185a22.png" alt="|L|"/> legitimate patterns closest to the root attack pattern, i.e., the initial attack pattern given to the gradient-descent algorithm. Since we employ a gaussian kernel, the KDE distance can be expressed as follows:</p>
<div class="math">
<p><img src="_images/math/51776aa1d0ddcd7a2d00ca64c5a173df64e18656.png" alt="d_{kde}(x, L) = \frac{1}{|L|}\sum_{y \in L} e^{-\gamma d(x,y)}"/></p>
</div><p>where <img class="math" src="_images/math/35bb7e062f2f35a2e924f8c2e89b50d61cfffb0b.png" alt="d(x,y)"/> is a generic distance measure between two patterns <img class="math" src="_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/> and <img class="math" src="_images/math/092e364e1d9d19ad5fffb0b46ef4cc7f2da02c1c.png" alt="y"/>.</p>
</div>
<div class="section" id="folders-and-file-format-gradient-descent-attack">
<span id="id8"></span><h4>Folders and File Format<a class="headerlink" href="#folders-and-file-format-gradient-descent-attack" title="Permalink to this headline">¶</a></h4>
<p>When a gradient-descent attack takes place, runtime information about the attack is printed in the standard output,
and a number of files are created. In particular, the folder:</p>
<ul>
<li><p class="first"><tt class="docutils literal"><span class="pre">classifiers/attack/gradient_descent</span></tt> contains all surrogate classifiers; filename format: <tt class="docutils literal"><span class="pre">&lt;surrogate_classifier_replica&gt;&#64;&lt;targeted_classifier_type&gt;.&lt;dataset_name&gt;.&lt;split_number&gt;.ts.cdat</span></tt>. <tt class="docutils literal"><span class="pre">&lt;surrogate_classifier_name&gt;</span></tt> is further structured as follows: <tt class="docutils literal"><span class="pre">&lt;surrogate_classifier_replica&gt;=&lt;surrogate_classifier_type&gt;-&lt;number_of_samples&gt;-&lt;repetition_number&gt;</span></tt>, where <tt class="docutils literal"><span class="pre">&lt;surrogate_classifier_type&gt;</span></tt> (analogously to <tt class="docutils literal"><span class="pre">&lt;targeted_classifier_type&gt;</span></tt> used for targeted classifiers) is a keyword specified in the setup file, that identifies: training algorithm (e.g. linear SVM), as well as training settings (see the <tt class="docutils literal"><span class="pre">SURROGATE_CLASSIFIER_PARAMS</span></tt> dictionary of the setup file, described in <a class="reference internal" href="#experimental-parameters">Experimental Parameters</a>). On the other hand, <tt class="docutils literal"><span class="pre">&lt;number_of_samples&gt;</span></tt> is the number of patterns used to train the <em>surrogate</em> classifier, <strong>randomly</strong> sampled from the corresponding testing split <tt class="docutils literal"><span class="pre">&lt;dataset_name&gt;.&lt;split_number&gt;.ts.txt</span></tt> within the <tt class="docutils literal"><span class="pre">datasets/base</span></tt> folder. Given a fixed number of training samples <tt class="docutils literal"><span class="pre">&lt;number_of_samples&gt;</span></tt>, <tt class="docutils literal"><span class="pre">&lt;repetition_number&gt;</span></tt> uniquely identifies a sampling process, i.e., multiple <em>surrogate</em> classifiers can be trained using the <em>same number</em> of samples, and the attack&#8217;s effectiveness can be averaged to limit bias errors on its evaluation.</p>
<blockquote>
<div><p><strong>NOTE</strong>: if a relabeling strategy is followed (see the option <tt class="docutils literal"><span class="pre">relabeling</span></tt>, described in <a class="reference internal" href="#experimental-parameters">Experimental Parameters</a>), all patterns used for training a <em>surrogate</em> classifier are labelled according to the output of the corresponding <em>targeted</em> classifier: <tt class="docutils literal"><span class="pre">&lt;targeted_classifier&gt;.&lt;dataset_name&gt;.&lt;split_number&gt;.tr.cdat</span></tt>.</p>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">datasets/attack/gradient_descent</span></tt> contains all computed attack patterns; filename format: <tt class="docutils literal"><span class="pre">&lt;surrogate_classifier_replica&gt;&#64;&lt;targeted_classifier_type&gt;.&lt;dataset_name&gt;.&lt;split_number&gt;.ts.txt</span></tt>. From the testing split <tt class="docutils literal"><span class="pre">&lt;dataset_name&gt;.&lt;split_number&gt;.ts.txt</span></tt> within <tt class="docutils literal"><span class="pre">datasets/base</span></tt>, we extract <em>all</em> attack patterns. Each attack pattern is numbered incrementally and then used as <strong>root</strong> (initial) pattern for the gradient-descent attack. Thus, each line in <tt class="docutils literal"><span class="pre">&lt;surrogate_classifier_replica&gt;&#64;&lt;targeted_classifier_type&gt;.&lt;dataset_name&gt;.&lt;split_number&gt;.ts.txt</span></tt> contains</dt>
<dd><ol class="first arabic simple">
<li>incremental number of the root attack pattern. This is necessary to uniquely identify each root pattern if multiprocessing is employed;</li>
<li>all attack patterns computed starting from such root pattern. For each main iteration, we store the corresponding attack pattern. We separate one attack pattern from its subsequent through a comma: &#8221;,&#8221; , whereas each feature pertaining to an attack pattern is space-separated.</li>
</ol>
<p class="last"><strong>NOTE</strong>: Please recall that attack patterns are computed applying the gradient-descent attack to the <em>surrogate classifier</em>, that may be either a <em>surrogate</em> or the <em>targeted</em> classifier itself.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">tests/attack/gradient_descent</span></tt> contains all scores related to attack patterns; filename format: <tt class="docutils literal"><span class="pre">&lt;targeted|surrogate&gt;_[&lt;surrogate_classifier_replica&gt;&#64;]&lt;targeted_classifier_type&gt;.&lt;dataset_name&gt;.&lt;split_number&gt;.ts.txt</span></tt>. Each line contains:</dt>
<dd><ol class="first arabic simple">
<li>incremental number of the root attack pattern, this is necessary to uniquely identify each root pattern if multiprocessing is employed;</li>
<li>the score of each attack patterns computed starting from such root pattern. For each main iteration, we store the score of the corresponding attack pattern. Scores are space-separated.</li>
</ol>
<p class="last"><strong>NOTE</strong>: If the filename has prefix <tt class="docutils literal"><span class="pre">targeted</span></tt> means that scores are related to the <em>targeted</em> classifier, otherwise (prefix <tt class="docutils literal"><span class="pre">surrogate</span></tt>) scores are related to the <em>surrogate</em> classifier. If the surrogate classifier corresponds to the targeted classifier (perfect knowledge) we skip the declaration of <tt class="docutils literal"><span class="pre">&lt;surrogate_classifier_replica&gt;&#64;</span></tt>, i.e., scores are saved within a filename with the following format: <tt class="docutils literal"><span class="pre">targeted_&lt;targeted_classifier_type&gt;.&lt;dataset_name&gt;.&lt;split_number&gt;.ts.txt</span></tt>.</p>
</dd>
</dl>
</li>
</ul>
</div>
</div>
<div class="section" id="experimental-parameters">
<span id="id9"></span><h3>Experimental Parameters<a class="headerlink" href="#experimental-parameters" title="Permalink to this headline">¶</a></h3>
<p>Here follows a description of experimental parameters, with reference to the <em>Gradient Descent Attack</em> whose settings are specified in <tt class="docutils literal"><span class="pre">experiments/svm_lin_attack/setup.py</span></tt>. By means of this example, we want to guide the interested reader to the definition of all experimental settings supported by the current version of <tt class="docutils literal"><span class="pre">AdversariaLib</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <span class="n">realpath</span><span class="p">,</span> <span class="n">dirname</span><span class="p">,</span> <span class="n">join</span><span class="p">,</span> <span class="n">abspath</span>
<span class="kn">from</span> <span class="nn">util.vars</span> <span class="kn">import</span> <span class="n">dotdictify</span><span class="p">,</span> <span class="n">BASE</span>

<span class="n">EXP_PATH</span> <span class="o">=</span> <span class="n">dirname</span><span class="p">(</span><span class="n">realpath</span><span class="p">(</span><span class="n">__file__</span><span class="p">))</span>
<span class="n">DSET_FOLDER</span> <span class="o">=</span> <span class="n">abspath</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">EXP_PATH</span><span class="p">,</span> <span class="s">&#39;../../dataset&#39;</span><span class="p">))</span>
<span class="n">DSET_NAME</span> <span class="o">=</span> <span class="s">&#39;norm_pdf_med&#39;</span>

<span class="n">TEST_FRACTION</span> <span class="o">=</span> <span class="mf">0.15</span>
<span class="n">NFOLDS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">NSPLITS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">CLASSIFIER_PARAMS</span> <span class="o">=</span> <span class="n">dotdictify</span><span class="p">({</span>
        <span class="s">&#39;SVM_lin&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s">&#39;lib&#39;</span><span class="p">:</span> <span class="s">&#39;sklearn.svm.SVC&#39;</span><span class="p">,</span>
                <span class="s">&#39;common&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s">&#39;kernel&#39;</span><span class="p">:</span> <span class="s">&#39;linear&#39;</span><span class="p">},</span>
                <span class="s">&#39;grid_search&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s">&#39;param_grid&#39;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">))},</span>
        <span class="p">},</span>
<span class="p">})</span>

<span class="n">GRID_PARAMS</span> <span class="o">=</span> <span class="n">dotdictify</span><span class="p">({</span><span class="s">&#39;iid&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="s">&#39;n_jobs&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>

<span class="n">NORM_WEIGHTS_FILEPATH</span> <span class="o">=</span> <span class="n">join</span><span class="p">(</span><span class="n">DSET_FOLDER</span><span class="p">,</span> <span class="s">&#39;norm_weights.txt&#39;</span><span class="p">)</span>
<span class="n">NORM_WEIGHTS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">float</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">NORM_WEIGHTS_FILEPATH</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()])</span>

<span class="n">SURROGATE_CLASSIFIER_PARAMS</span> <span class="o">=</span> <span class="n">CLASSIFIER_PARAMS</span>

<span class="n">ATTACK_PARAMS</span> <span class="o">=</span> <span class="n">dotdictify</span><span class="p">({</span>
        <span class="s">&#39;gradient_descent&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s">&#39;attack_class&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">&#39;maxiter&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
                <span class="s">&#39;score_threshold&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">&#39;step&#39;</span><span class="p">:</span><span class="mf">0.01</span><span class="p">,</span>
                <span class="s">&#39;fname_metachar_attack_vs_fname_metachar_surrogate_vs_targeted&#39;</span><span class="p">:</span> <span class="s">&#39;@&#39;</span><span class="p">,</span>
                <span class="s">&#39;fname_metachar_samples_repetitions&#39;</span><span class="p">:</span> <span class="s">&#39;-&#39;</span><span class="p">,</span>
                <span class="s">&#39;constraint_function&#39;</span><span class="p">:</span> <span class="s">&#39;only_increment&#39;</span><span class="p">,</span> <span class="s">&#39;constraint_params&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s">&#39;only_increment_step&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
                <span class="s">&#39;stop_criteria_window&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span> <span class="s">&#39;stop_criteria_epsilon&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">9</span><span class="p">),</span> <span class="s">&#39;max_boundaries&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
                <span class="s">&#39;lambda_value&#39;</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="s">&#39;mimicry_distance&#39;</span><span class="p">:</span> <span class="s">&#39;kde_hamming&#39;</span><span class="p">,</span> <span class="s">&#39;relabeling&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
                <span class="s">&#39;mimicry_params&#39;</span> <span class="p">:</span> <span class="p">{</span><span class="s">&#39;max_leg_patterns&#39;</span><span class="p">:</span> <span class="mi">100</span> <span class="p">,</span> <span class="s">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">},</span>
                <span class="s">&#39;save_attack_patterns&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="s">&#39;threads&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                <span class="s">&#39;training&#39;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="s">&#39;dataset_knowledge&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s">&#39;samples_range&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span> <span class="s">&#39;repetitions&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
                <span class="p">},</span>
        <span class="p">},</span>
<span class="p">})</span>
</pre></div>
</div>
<ul>
<li><p class="first"><tt class="docutils literal"><span class="pre">EXP_PATH</span></tt>: path of the main folder of the experiment (where <tt class="docutils literal"><span class="pre">setup.py</span></tt> is stored)</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">DSET_FOLDER</span></tt>: absolute path of the folder containing the dataset; in this case, we go two levels up with respect the folder containing <tt class="docutils literal"><span class="pre">setup.py</span></tt> (i.e., <tt class="docutils literal"><span class="pre">EXP_PATH</span></tt>) to find the <tt class="docutils literal"><span class="pre">ECML_Dataset</span></tt> folder.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">DSET_NAME</span></tt>: name of the dataset, excluding its <cite>.txt</cite> extension. Each sample within dataset <tt class="docutils literal"><span class="pre">norm_pdf_med</span></tt> is related to either legitimate (class label=-1) or malicious (class label=1) PDF files. Each PDF sample is described by one line where each feature is space-separated and the last value indicates its class label. Each feature is the <em>normalized</em> occurrence (count) of a specific name object within a PDF document. More details about this dataset are available in our research publication <a class="reference internal" href="#ecml2013">[ECML2013]</a>.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">TEST_FRACTION</span></tt>: dataset fraction reserved for testing classifiers. Testing data is <em>randomly</em> sampled from the original dataset <tt class="docutils literal"><span class="pre">NSPLITS</span></tt> times.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">NSPLITS</span></tt>: please, see <tt class="docutils literal"><span class="pre">TEST_FRACTION</span></tt>.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">NFOLDS</span></tt>: number of folds to be used for cross-validation on training data.</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">CLASSIFIER_PARAMS</span></tt>: a <a class="reference external" href="http://docs.python.org/2/library/stdtypes.html">python dictionary</a>, where each key-value pair describes a targeted classifier to be built. Each classifier is characterized by a unique key (e.g. <tt class="docutils literal"><span class="pre">'SVM_lin'</span></tt>) and a value (dictionary) specifying its training settings.  In particular, such a dictionary must specify the following keys:</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">'lib'</span></tt>: the base class for the classifier (e.g. <a class="reference external" href="http://scikit-learn.org/stable/modules/svm.html#svc">sklearn.svm.SVC</a>). Please note that this string will be used internally to <tt class="docutils literal"><span class="pre">import</span></tt> the class for building the classifier (e.g. <tt class="docutils literal"><span class="pre">'lib':</span> <span class="pre">'sklearn.svm.SVC'</span></tt> corresponds to the python instruction <tt class="docutils literal"><span class="pre">from</span> <span class="pre">sklearn.svm</span> <span class="pre">import</span> <span class="pre">SVC</span></tt> and a <tt class="docutils literal"><span class="pre">SVC</span></tt> object will be instantiated). Thanks to this approach, <tt class="docutils literal"><span class="pre">adversariaLib</span></tt>:</dt>
<dd><ul class="first last simple">
<li>can support all classifiers defined in <tt class="docutils literal"><span class="pre">scikit-learn</span></tt>, accross multiple versions</li>
<li>can support any other <a class="reference external" href="http://docs.python.org/2/library/pickle.html">Pickable</a> classifier that provides at least the following <tt class="docutils literal"><span class="pre">scikit-learn</span></tt>-compatible methods <tt class="xref py py-func docutils literal"><span class="pre">fit()</span></tt>, <tt class="xref py py-func docutils literal"><span class="pre">predict()</span></tt> and <tt class="xref py py-func docutils literal"><span class="pre">decision_function()</span></tt>. See, for instance, our MLP wrapper for <tt class="docutils literal"><span class="pre">scikit-learn</span></tt> in <tt class="docutils literal"><span class="pre">adversariaLib/prlib/classifier/mlp.py</span></tt>. See also how this classifier is referenced in <tt class="docutils literal"><span class="pre">experiments/mlp_attack/setup.py</span></tt>.</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'common'</span></tt>: parameters of the classifier&#8217;s constructor. Such parameters are passed using a dictionary where each key-value is <em>attribute name</em> (i.e., a <em>string</em>) and its <em>value</em>. For instance, considering the above setup, <tt class="docutils literal"><span class="pre">'common':</span> <span class="pre">{'kernel':</span> <span class="pre">'linear'}</span></tt> will lead to the following object instantiation <tt class="docutils literal"><span class="pre">SVC(kernel='linear')</span></tt>.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'grid_search'</span></tt>: classifier settings for cross-validation. All attributes specified for <a class="reference external" href="http://scikit-learn.org/dev/modules/generated/sklearn.grid_search.GridSearchCV.html">sklearn.grid_search.GridSearchCV</a> are supported, however, the most important attribute is <tt class="docutils literal"><span class="pre">param_grid</span></tt>. The value of this attribute should be a dictionary where, for each classifier parameter, an array of values to be tested is specified. For instance, considering the above setup, <tt class="docutils literal"><span class="pre">dict(C=np.logspace(-3,</span> <span class="pre">2,</span> <span class="pre">6))</span></tt> corresponds to the dictionary <tt class="docutils literal"><span class="pre">{'C':</span> <span class="pre">array([1e-03,</span> <span class="pre">1e-02,</span> <span class="pre">1e-01,</span> <span class="pre">1e+00,</span> <span class="pre">1e+01,</span> <span class="pre">1e+02])}</span></tt> (see <a class="reference external" href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.logspace.html">numpy.logspace</a>), which means that during the cross-validation process, the classifier accuracy will be evaluated considering the following values <tt class="docutils literal"><span class="pre">C</span> <span class="pre">in</span> <span class="pre">[0.001,</span> <span class="pre">0.01,</span> <span class="pre">0.1,</span> <span class="pre">1,</span> <span class="pre">10,</span> <span class="pre">100]</span></tt>.</p>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">GRID_PARAMS</span></tt>: this parameter has the same objective of <tt class="docutils literal"><span class="pre">param_grid</span></tt>, but it is useful to define cross-validation settings that are in common between all classifiers. In particular, considering the above setup file, we can define (see <a class="reference external" href="http://scikit-learn.org/dev/modules/generated/sklearn.grid_search.GridSearchCV.html">sklearn.grid_search.GridSearchCV</a> for more details):</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">'iid'</span></tt>: must be set to <tt class="docutils literal"><span class="pre">True</span></tt> or <tt class="docutils literal"><span class="pre">False</span></tt>. If we set <tt class="docutils literal"><span class="pre">True</span></tt>, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds.</li>
<li><tt class="docutils literal"><span class="pre">'n_jobs'</span></tt>: concurrent threads (jobs) for cross-validation. Setting a value higher than 1, can significantly speed up cross-validation on systems with multiple processors.</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">NORM_WEIGHTS_FILEPATH</span></tt>:  path of the file containing normalization weights, i.e., a (space-separated) integer value for each feature. Original patterns can be obtained multiplying each normalized feature value by its corresponding weight. Given <tt class="docutils literal"><span class="pre">NORM_WEIGHTS_FILEPATH</span></tt>, we extract <tt class="docutils literal"><span class="pre">NORM_WEIGHTS</span></tt>, i.e., the actual array of normalization weights. It is an array containing a list of normalization weights for patterns within the loaded dataset. Such weights are such that multiplying element-by-element <tt class="docutils literal"><span class="pre">NORM_WEIGHTS</span></tt> for each (normalized) pattern within the dataset, we get back the original (unnormalized) feature vector. This variable is optional, and if not specified, each feature will receive the same weight during the gradient-descent attack.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">NORM_WEIGHTS</span></tt>: please, see <tt class="docutils literal"><span class="pre">NORM_WEIGHTS_FILEPATH</span></tt>.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">SURROGATE_CLASSIFIER_PARAMS</span></tt>: dictionary analogous to <tt class="docutils literal"><span class="pre">CLASSIFIER_PARAMS</span></tt>, but dedicated to <em>surrogate</em> classifiers, i.e., classifiers built using a (random) portion of testing data (see <tt class="docutils literal"><span class="pre">TEST_FRACTION</span></tt>). These <em>surrogate</em> classifiers can be used to emulate the behavior of an adversary who has partial knowledge of the target classifier, but can sample a portion of <em>testing</em> data during its operational phase.</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">ATTACK_PARAMS</span></tt>: a dictionary where each key-value pair describes a machine-learning attack to be performed. Each key of this dictionary uniquely identifies an attack technique, whereas the related value defines all attack parameters in the form of a (sub)dictionary. Please note that <strong>all</strong> attack settings might specify a <tt class="docutils literal"><span class="pre">'training'</span></tt> parameter (and the related sub-paramaters, as described in the following), whereas all other parameters are <em>attack-specific</em>. Currently, <tt class="docutils literal"><span class="pre">adversariaLib</span></tt> comes with <tt class="docutils literal"><span class="pre">'gradient_descent'</span></tt> (<tt class="docutils literal"><span class="pre">adversariaLib/advlib/attacks/gradient_descent/gradient_descent.py</span></tt>), that is, the implementation of the <em>Gradient Descent Attack</em> described in <a class="reference internal" href="#ecml2013">[ECML2013]</a>. New attacks can be easily added to the library, inserting their implementation within folder <tt class="docutils literal"><span class="pre">adversariaLib/advlib/attacks/</span></tt>. Each <em>gradient descent</em> attack setting may define the following keys:</dt>
<dd><ul class="first last">
<li><p class="first"><tt class="docutils literal"><span class="pre">'attack_class'</span></tt>: label of attack patterns, in this case, <tt class="docutils literal"><span class="pre">1</span></tt>. This parameter is needed to identify all attack patterns within all testing portions of the dataset.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'maxiter'</span></tt>: number of maximum modifications of a root pattern during the attack</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'score_threshold'</span></tt>: score threshold of the attacked classifier. It may be a surrogate classifier (partial knowledge) or the targeted classifier (full knowledge).</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'step'</span></tt>: incremental step at each iteration along the feature-space direction which minimizes the gradient. It is exacltly the <em>t</em> parameter described in <a class="reference internal" href="#ecml2013">[ECML2013]</a>.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'max_boundaries'</span></tt>: defines the maximum number of <em>main</em> iterations to be performed. Let <tt class="docutils literal"><span class="pre">boundary_number</span> <span class="pre">in</span> <span class="pre">[1,max_boundaries]</span></tt> be an integer that identifies each main iteration of the gradient-descent algorithm. Each main iteration corresponds to a different <tt class="docutils literal"><span class="pre">boundary_number</span></tt>, i.e., a different constraint. The higher the <tt class="docutils literal"><span class="pre">boundary_number</span></tt>, the less the constraint on the distance between a computed attack pattern and the root (initial) attack pattern given to the gradient-descent algorithm. Such distance is defined (and enforced) by the <tt class="docutils literal"><span class="pre">'constraint_function'</span></tt> option.</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">'constraint_function'</span></tt>: specifies the function that applies a constraint to each attack pattern computed by the gradient-descent attack. These functions are implemented in <tt class="docutils literal"><span class="pre">adversariaLib/advlib/attacks/gradient_descent/constraints.py</span></tt>. The current implementation of <tt class="docutils literal"><span class="pre">adversariaLib</span></tt> comes with the following options:</dt>
<dd><ul class="first last">
<li><p class="first"><tt class="docutils literal"><span class="pre">None</span></tt>: no constraints are applied to attack patterns, i.e., all computed attack patterns are accepted. This option is useful for testing purposes</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">'box'</span></tt>: forces each attack pattern to be confined within a hypercube centered on the root attack pattern. It receives one attribute (specified through the <tt class="docutils literal"><span class="pre">'constraint_params'</span></tt> parameter, described in the following):</dt>
<dd><ul class="first simple">
<li><tt class="docutils literal"><span class="pre">'box_step'</span></tt>: this defines the box side dimension at each iteration: <tt class="docutils literal"><span class="pre">box</span> <span class="pre">side</span> <span class="pre">=</span> <span class="pre">box_step</span> <span class="pre">*</span> <span class="pre">boundary_number</span></tt> (see the <tt class="docutils literal"><span class="pre">'max_boundaries'</span></tt> option).</li>
</ul>
<p class="last"><strong>NOTE</strong>: This option currently does not handle normalization weights, thus it is suggested to use it on a unnormalized dataset. Hence, if you select this option, you should <em>not</em> specify the <tt class="docutils literal"><span class="pre">NORM_WEIGHTS</span></tt> variable.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">'hamming'</span></tt>: forces each attack pattern to be confined within <tt class="docutils literal"><span class="pre">hamming</span> <span class="pre">distance</span> <span class="pre">=</span> <span class="pre">boundary_number</span></tt> from the root attack pattern (see the <tt class="docutils literal"><span class="pre">'max_boundaries'</span></tt> option).</dt>
<dd><p class="first last"><strong>NOTE</strong>: This option does not need normalization weights, since it is expected to work with binary features. Thus, if you select this option, you should <em>not</em> specify the <tt class="docutils literal"><span class="pre">NORM_WEIGHTS</span></tt> variable.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">'only_increment'</span></tt>: each feature of an attack pattern must have a value higher or equal to that of the root attack pattern. This reflects a real constraint of PDF samples in <tt class="docutils literal"><span class="pre">'norm_pdf_med'</span></tt>. That is, an attacker can easily <strong>add</strong> arbitrary <em>name objects</em> to a PDF file (by means of the <em>Adobe PDF versioning mechanism</em>), while it is much more difficult to <strong>remove</strong> them, to evade detection and retain the PDF&#8217;s original malicious behavior. Such constraint function receives the following attributes (specified through the <tt class="docutils literal"><span class="pre">'constraint_params'</span></tt> parameter, described in the following):</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">'only_increment_step'</span></tt>: it defines the maximum number of feature increments, according to: <tt class="docutils literal"><span class="pre">feature_increments</span> <span class="pre">=</span> <span class="pre">only_increment_step</span> <span class="pre">*</span> <span class="pre">boundary_number</span></tt> (see the <tt class="docutils literal"><span class="pre">'max_boundaries'</span></tt> option).</li>
<li><tt class="docutils literal"><span class="pre">'feature_upper_bound'</span></tt> (integer, optional): it forces an upper bound to each feature of attack patterns (in the <strong>unnormalized</strong> space)</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'constraint_params'</span></tt>: this parameter specifies the list of attributes required by the selected <tt class="docutils literal"><span class="pre">'constraint_function'</span></tt> option. If no attributes are required, an empty dictionary should be specified, i.e., <tt class="docutils literal"><span class="pre">'constraint_params':</span> <span class="pre">{}</span></tt></p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'stop_criteria_window'</span></tt>: number of elements of the array employed by the stop criteria. In the above setup example, this option has value <tt class="docutils literal"><span class="pre">20</span></tt>, which means that the gradient-descent algorithm <strong>stops</strong> if within 20 iterations (not to be confused with main iterations) the decrement of the objective function is less than <tt class="docutils literal"><span class="pre">'stop_criteria_epsilon'</span></tt>.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'stop_criteria_epsilon'</span></tt>: it corresponds to the <img class="math" src="_images/math/eaf4418fbe935c15a606516d8f55dc380cd8e822.png" alt="\epsilon"/> parameter of the algorithm <a class="reference internal" href="#ecml2013">[ECML2013]</a>, and defines a little constant that is used to stop the algorithm if the objective function did not decrease significantly within the last <tt class="docutils literal"><span class="pre">'stop_criteria_window'</span></tt> iterations</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'lambda_value'</span></tt>: it corresponds to the <img class="math" src="_images/math/ce4588fd900d02afcbd260bc07f54cce49a7dc4a.png" alt="\lambda"/> parameter of the algorithm <a class="reference internal" href="#ecml2013">[ECML2013]</a>, and reflects the tradeoff between density-based component (essentially this depends on training data only) and classifier component (it depends on the parameters of the <em>surrogate</em> classifier) when defining the objective function</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">'mimicry_distance'</span></tt>: it defines the function that should be used to calculate the distance between an attack pattern and legitimate patterns. Please note that, for performance reasons, only subset of <tt class="docutils literal"><span class="pre">'max_leg_patterns'</span></tt> legitimate patterns closest to the root attack pattern, are considered. These functions are implemented in <tt class="docutils literal"><span class="pre">adversariaLib/advlib/attacks/gradient_descent/gradient_distances.py</span></tt>. The current implementation of <tt class="docutils literal"><span class="pre">adversariaLib</span></tt> comes with the following options:</dt>
<dd><ul class="first simple">
<li><tt class="docutils literal"><span class="pre">'euclidean'</span></tt>: calculates the distance between an attack pattern and its closest legitimate pattern.</li>
<li><tt class="docutils literal"><span class="pre">'kde_euclidean'</span></tt>: calculates the distance between an attack pattern and the distribution of legitimate patterns, estimated through kernel density estimation, where <img class="math" src="_images/math/35bb7e062f2f35a2e924f8c2e89b50d61cfffb0b.png" alt="d(x,y)"/> is the <em>euclidean</em> distance between pattern <img class="math" src="_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/> and <img class="math" src="_images/math/092e364e1d9d19ad5fffb0b46ef4cc7f2da02c1c.png" alt="y"/>. See the above section on <a class="reference internal" href="#kde-distance">KDE distance</a>.</li>
<li><tt class="docutils literal"><span class="pre">'kde_hamming'</span></tt>: calculates the distance between an attack pattern and the distribution of legitimate patterns, estimated through kernel density estimation, where <img class="math" src="_images/math/35bb7e062f2f35a2e924f8c2e89b50d61cfffb0b.png" alt="d(x,y)"/> is the <em>hamming</em> aka <em>manhattan</em> aka <img class="math" src="_images/math/d89a5fe07fe3854a174b684785a2d0eb36f2b232.png" alt="l1"/> distance between two patterns <img class="math" src="_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/> and <img class="math" src="_images/math/092e364e1d9d19ad5fffb0b46ef4cc7f2da02c1c.png" alt="y"/>. See the above section on <a class="reference internal" href="#kde-distance">KDE distance</a>.</li>
</ul>
<p class="last">All functions accept the attribute <tt class="docutils literal"><span class="pre">'max_leg_patterns'</span></tt>, whereas functions <tt class="docutils literal"><span class="pre">'kde_euclidean'</span></tt> and <tt class="docutils literal"><span class="pre">'kde_hamming'</span></tt> accept also the <tt class="docutils literal"><span class="pre">'gamma'</span></tt> attribute (it shapes the gaussian kernel, see the <img class="math" src="_images/math/66981fa3920210c6ad8dbe5e968783d5dd7520c3.png" alt="\gamma"/> parameter of the <a class="reference internal" href="#kde-distance">KDE distance</a>). They all are optional, and can be specified by means of the <tt class="docutils literal"><span class="pre">'mimicry_params'</span></tt> parameter (see below for default values).</p>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'relabeling'</span></tt>: it may be <tt class="docutils literal"><span class="pre">True</span></tt> or <tt class="docutils literal"><span class="pre">False</span></tt>. If <tt class="docutils literal"><span class="pre">True</span></tt>, all patterns used for training <em>surrogate</em> classifiers are labelled according to the output of the corresponding <em>targeted</em> classifier. In this case, we are emulating an adversary who actively queries the targeted classifier to re-label each pattern before training a <em>surrogate</em> classifier.</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">'mimicry_params'</span></tt>: attributes associate to the chosen <tt class="docutils literal"><span class="pre">'mimicry_distance'</span></tt> option. In the above <tt class="docutils literal"><span class="pre">setup.py</span></tt>, we have set:</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">'max_leg_patterns'</span></tt>: <tt class="docutils literal"><span class="pre">100</span></tt> (default value: <tt class="docutils literal"><span class="pre">10</span></tt>)</li>
<li><tt class="docutils literal"><span class="pre">'gamma'</span></tt>: 0.001  (default value: <tt class="docutils literal"><span class="pre">0.1</span></tt>)</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">'save_attack_patterns'</span></tt>: it may be <tt class="docutils literal"><span class="pre">True</span></tt> or <tt class="docutils literal"><span class="pre">False</span></tt>.  If <tt class="docutils literal"><span class="pre">True</span></tt>, all computed attack patterns are saved (within the folder <tt class="docutils literal"><span class="pre">dataset/attack/gradient_descent/</span></tt>).</dt>
<dd><p class="first last"><strong>NOTE</strong>: if you set  <tt class="docutils literal"><span class="pre">'save_attack_patterns':</span> <span class="pre">True</span></tt> and <tt class="docutils literal"><span class="pre">'max_boundaries':</span> <span class="pre">1</span></tt>, all attack patterns, related to the <em>base</em> iterations performed until a local minimum is reached, will be saved. This behavior is useful for testing purposes.</p>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'threads'</span></tt>: number of concurrent processes to be used for the gradient-descent attack. Values higher than one may greatly speedup the experimental process. If we set <tt class="docutils literal"><span class="pre">-1</span></tt>, <tt class="docutils literal"><span class="pre">adversariaLib</span></tt> will launch as many processes as the number of CPUs of the machine (maximum theoretical speed).</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">'training'</span></tt>: (optional) this section is dedicated to the definition of training parameters for <em>surrogate</em> classifiers</dt>
<dd><ul class="first">
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">'dataset_knowledge'</span></tt>: this section is dedicated to the definition of the dataset knowledge gained by the adversary</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">'samples_range'</span></tt>: a <a class="reference external" href="http://docs.python.org/2/tutorial/datastructures.html">python list</a> where each element is the number of samples to be used for training a <em>surrogate</em> classifier. An easy way to define such list is by means of the <tt class="docutils literal"><span class="pre">range(start,stop,step)</span></tt> function. Please, note that the stop value is never included in the list. For instance: <tt class="docutils literal"><span class="pre">range(50,250,50)</span></tt> produces a list with four elements: <tt class="docutils literal"><span class="pre">[50,</span> <span class="pre">100,</span> <span class="pre">150,</span> <span class="pre">200]</span></tt>, whereas <tt class="docutils literal"><span class="pre">range(50,250,50)</span></tt> produces a list with a unique element: <tt class="docutils literal"><span class="pre">[50]</span></tt>.</li>
<li><tt class="docutils literal"><span class="pre">'repetitions'</span></tt>: number of classifiers to be trained (repetitions) for each number (element) specified in the <tt class="docutils literal"><span class="pre">'samples_range'</span></tt> list. Please note that each repetition prefigures random sampling of a fixed number of elements from the same testing split.</li>
</ul>
</dd>
</dl>
</li>
</ul>
<p class="last"><strong>NOTE</strong>: when <tt class="docutils literal"><span class="pre">training</span></tt> is specified, the algorithm automatically considers the dictionary <tt class="docutils literal"><span class="pre">SURROGATE_CLASSIFIER_PARAMS</span></tt>, that specifies what learning algoritms should be used to build <em>surrogate</em> classifiers. In the above <tt class="docutils literal"><span class="pre">setup.py</span></tt> we have exactly the same dictionary, thus <em>surrogate</em> classifiers will be built using the same learning algorithm of targeted classifiers (i.e., linear SVM). Please note that the number of <em>surrogate</em> classifiers built is thus given by the number of algorithms specified in this dictionary (i.e., the number of <strong>keys</strong>), multiplied by the number of elements in <tt class="docutils literal"><span class="pre">'samples_range'</span></tt>, multiplied by the number of <tt class="docutils literal"><span class="pre">'repetitions'</span></tt>, multiplied by the number of (testing) splits <tt class="docutils literal"><span class="pre">NSPLITS</span></tt>. Thus, each <em>surrogate</em> classifier is uniquely identified (thorugh its file name) given its: (a) training algorithm key (e.g. SVM_lin), (b) number of training samples, (c) number of the sampling repetition, (d) testing split.</p>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'fname_metachar_surrogate_vs_targeted'</span></tt>: This attribute specifies what meta-character should be used to separate the name of the <em>surrogate classifier</em> and that of the <em>target classifier</em> in a file name. In the section: <a class="reference internal" href="#folders-and-file-format-gradient-descent-attack">Folders and File Format - Gradient Descent Attack</a>, as well as, in the above <tt class="docutils literal"><span class="pre">setup.py</span></tt>, we employed a <tt class="docutils literal"><span class="pre">'&#64;'</span></tt> meta-character.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">'fname_metachar_samples_repetitions'</span></tt>: This attribute specifies what meta-character should be used to separate number of samples and the number of the repetition, within the name of <em>surrogate</em> classifiers. In the section: <a class="reference internal" href="#folders-and-file-format-gradient-descent-attack">Folders and File Format - Gradient Descent Attack</a>, as well as, in the above <tt class="docutils literal"><span class="pre">setup.py</span></tt>, we employed a <tt class="docutils literal"><span class="pre">'-'</span></tt> meta-character.</p>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
</div>
<div class="section" id="developers-section">
<span id="id15"></span><h2>Developers Section<a class="headerlink" href="#developers-section" title="Permalink to this headline">¶</a></h2>
<p>This section is of interest to anyone who wants to extend/improve adversariaLib. Are you interested in? You can <a class="reference internal" href="#contribute">Contribute</a>.</p>
<div class="section" id="adding-new-attacks">
<h3>Adding New Attacks<a class="headerlink" href="#adding-new-attacks" title="Permalink to this headline">¶</a></h3>
<p>Each attack must be identified by a unique name (with no space characters) within adversariaLib.
An attack can be launched by specifying its name as keyword of the <tt class="docutils literal"><span class="pre">ATTACK_PARAMS</span></tt> dictionary, within a <tt class="docutils literal"><span class="pre">setup.py</span></tt> file.
This is because the <tt class="docutils literal"><span class="pre">runexp.py</span></tt> script considers each key specified in <tt class="docutils literal"><span class="pre">ATTACK_PARAMS</span></tt> as the name of the attack function
that can be loaded from the <tt class="docutils literal"><span class="pre">advlib/attacks</span></tt> folder.</p>
<p>For instance, let us refer to the setup file <tt class="docutils literal"><span class="pre">experiments/svm_lin_attack/setup.py</span></tt>.
The runexp.py script automatically tries to perform the following import:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">advlib.attacks</span> <span class="kn">import</span> <span class="n">gradient_descent</span>
</pre></div>
</div>
<p>since <tt class="docutils literal"><span class="pre">ATTACK_PARAMS</span></tt> contains the key <tt class="docutils literal"><span class="pre">'gradient_descent'</span></tt>. Let us refer to <tt class="docutils literal"><span class="pre">attack_name</span></tt> as the name of the function that implements an attack.
In general, each <tt class="docutils literal"><span class="pre">attack_name</span></tt> function must define the following general interface:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">attack_name</span><span class="p">(</span><span class="n">setup</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">surrogate_training_indexes</span><span class="p">,</span> <span class="n">surrogate_classifier</span><span class="p">,</span> <span class="n">targeted_classifier</span><span class="p">,</span> <span class="n">attack_params</span><span class="p">,</span> <span class="n">fnames</span><span class="p">):</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>where:</p>
<ul>
<li><p class="first"><tt class="docutils literal"><span class="pre">setup</span></tt> is the main setup object, that can be used to retrieve any variable specified in <tt class="docutils literal"><span class="pre">setup.py</span></tt></p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">X</span></tt> is the whole set of patterns in the current testing split (<tt class="docutils literal"><span class="pre">numpy.array</span></tt> instance)</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">y</span></tt> is the whole set of true labels for patterns in <tt class="docutils literal"><span class="pre">X</span></tt> (<tt class="docutils literal"><span class="pre">numpy.array</span></tt> instance)</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">target_classifier</span></tt>: the target classifier (a <a class="reference external" href="http://scikit-learn.org/stable/supervised_learning.html">scikit-learn</a> classifier)</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">y_t</span></tt>: the whole set of labels assigned by <tt class="docutils literal"><span class="pre">target_classifier</span></tt> to each pattern in <tt class="docutils literal"><span class="pre">X</span></tt> (<tt class="docutils literal"><span class="pre">numpy.array</span></tt> instance).</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">surrogate_classifier</span></tt>: the surrogate classifier that should be attacked (a <a class="reference external" href="http://scikit-learn.org/stable/supervised_learning.html">scikit-learn</a> classifier)</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">surrogate_training_indexes</span></tt>: the array of indexes of patterns in <tt class="docutils literal"><span class="pre">X</span></tt> employed for building the surrogate classifier</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">attack_params</span></tt>: all parameters specified in the setup file. It corresponds to <tt class="docutils literal"><span class="pre">setup.ATTACK_PARAMS.attack_name</span></tt></p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">fnames</span></tt>: contains different filenames</dt>
<dd><ul class="first last simple">
<li><tt class="docutils literal"><span class="pre">fnames.surrogate_score</span></tt>: full path of the file where <tt class="docutils literal"><span class="pre">attack_name</span></tt> should store the score of the surrogate classifier associated to each attack iteration (for instance, <tt class="docutils literal"><span class="pre">gradient_descent</span></tt> stores the score assigned by the surrogate classifier to the attack pattern computed at each iteration)</li>
<li><tt class="docutils literal"><span class="pre">fnames.targeted_score</span></tt>: full path of the file where <tt class="docutils literal"><span class="pre">attack_name</span></tt> should store the score of the targeted classifier associated to each attack iteration  (for instance, <tt class="docutils literal"><span class="pre">gradient_descent</span></tt> stores the score assigned by the targeted classifier to the attack pattern computed at each iteration)</li>
<li><tt class="docutils literal"><span class="pre">fnames.data</span></tt>: full path of the file where <tt class="docutils literal"><span class="pre">attack_name</span></tt> should store other data about the attack for each iteration (for instance, <tt class="docutils literal"><span class="pre">gradient_descent</span></tt> stores the attack pattern computed at each iteration)</li>
</ul>
</dd>
</dl>
</li>
</ul>
<p><tt class="docutils literal"><span class="pre">adversariaLib</span></tt> automatically submits such variables to each attack function.</p>
</div>
<div class="section" id="adding-new-classifiers">
<h3>Adding new classifiers<a class="headerlink" href="#adding-new-classifiers" title="Permalink to this headline">¶</a></h3>
<p>It is straightforward to add new classifiers to <tt class="docutils literal"><span class="pre">adversariaLib</span></tt>. This operation is useful to evaluate classification algorithms that are not implemented in <a class="reference external" href="http://scikit-learn.org/stable/supervised_learning.html">scikit-learn</a>, for instance, robust versions of a particular classification algorithm.
In order to be recognized by <a class="reference external" href="http://scikit-learn.org/stable/supervised_learning.html">scikit-learn</a> each classifier should</p>
<ul>
<li><p class="first">be a subclass of <tt class="docutils literal"><span class="pre">BaseEstimator</span></tt> and <tt class="docutils literal"><span class="pre">ClassifierMixin</span></tt> classes defined in <a class="reference external" href="http://scikit-learn.org/stable/supervised_learning.html">scikit-learn</a> (<tt class="docutils literal"><span class="pre">sklearn.base</span></tt>)</p>
</li>
<li><p class="first">implement the following methods:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">fit(X,y)</span></tt> that trains the classifier using the specified patterns <tt class="docutils literal"><span class="pre">X</span></tt> and their labels <tt class="docutils literal"><span class="pre">y</span></tt></li>
<li><tt class="docutils literal"><span class="pre">predict(x)</span></tt>, that returns the label predicted by the classifier to pattern <tt class="docutils literal"><span class="pre">x</span></tt></li>
<li><tt class="docutils literal"><span class="pre">decision_function(x)</span></tt>, that returns the score assigned by the classifier to pattern <tt class="docutils literal"><span class="pre">x</span></tt></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>In the following we provide a general template for the definition of new classifiers that are recognized by <a class="reference external" href="http://scikit-learn.org/stable/supervised_learning.html">scikit-learn</a>. You should save the related <tt class="docutils literal"><span class="pre">.py</span></tt> file within the <tt class="docutils literal"><span class="pre">prlib/classifier</span></tt> folder of the library.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span>
<span class="k">class</span> <span class="nc">new_classifier</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">init_params</span><span class="p">):</span>
                <span class="o">...</span>
        <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
                <span class="o">...</span>
        <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
                <span class="o">...</span>
        <span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
                <span class="o">...</span>
</pre></div>
</div>
<p>You might also take a look to our <a class="reference external" href="http://scikit-learn.org/stable/supervised_learning.html">scikit-learn</a> wrapper for <a class="reference external" href="http://leenissen.dk/fann/wp/">FANN</a> classifiers also known as Multi-Layer Perceptron, implemented in <tt class="docutils literal"><span class="pre">prlib/classifier/mlp.py</span></tt>.</p>
</div>
</div>
<div class="section" id="help-and-suggestions">
<span id="getting-help-and-sending-suggestions"></span><h2>Help and Suggestions<a class="headerlink" href="#help-and-suggestions" title="Permalink to this headline">¶</a></h2>
<p>Did you encounter any problem? Did you find any bug? Would you like to send suggestions? Please use the <a class="reference external" href="https://sourceforge.net/p/adversarialib/discussion/?source=navbar">forum on the official repository of adversariaLib</a>.</p>
</div>
<div class="section" id="contribute">
<span id="id16"></span><h2>Contribute<a class="headerlink" href="#contribute" title="Permalink to this headline">¶</a></h2>
<p>Are you a researcher working on adversarial Machine Learning? Would you like to contribute to this project? Feel free to contact us by email:</p>
<ul class="simple">
<li><a class="reference external" href="http://pralab.diee.unica.it/en/IginoCorona">Igino Corona</a>, <a class="reference external" href="http://pralab.diee.unica.it/en/BattistaBiggio">Battista Biggio</a>. Mail to: <tt class="docutils literal"><span class="pre">&lt;name&gt;.&lt;surname&gt;&#64;diee.unica.it</span></tt>.</li>
</ul>
</div>
<div class="section" id="snapshots">
<span id="id17"></span><h2>Snapshots<a class="headerlink" href="#snapshots" title="Permalink to this headline">¶</a></h2>
<img alt="_images/attack.png" src="_images/attack.png" />
</div>
<div class="section" id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h2>
<table class="docutils citation" frame="void" id="ecml2013" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[ECML2013]</td><td><em>(<a class="fn-backref" href="#id5">1</a>, <a class="fn-backref" href="#id10">2</a>, <a class="fn-backref" href="#id11">3</a>, <a class="fn-backref" href="#id12">4</a>, <a class="fn-backref" href="#id13">5</a>, <a class="fn-backref" href="#id14">6</a>)</em> Biggio B., Corona I., Maiorca D., Nelson B., Srndic N., Laskov P., Giacinto G., Roli F., <a class="reference download internal" href="_downloads/Biggio13-ecml.pdf"><tt class="xref download docutils literal"><span class="pre">Evasion</span> <span class="pre">attacks</span> <span class="pre">against</span> <span class="pre">machine</span> <span class="pre">learning</span> <span class="pre">at</span> <span class="pre">test</span> <span class="pre">time</span></tt></a>, in <a class="reference external" href="http://www.ecmlpkdd2013.org">European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases* (ECML-PKDD 2013)</a>, Prague, Czech Republic, 2013.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">AdversariaLib&#8217;s documentation</a><ul>
<li><a class="reference internal" href="#adversarial-machine-learning">Adversarial Machine Learning</a></li>
<li><a class="reference internal" href="#about-adversarialib">About AdversariaLib</a></li>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#usage-examples">Usage Examples</a><ul>
<li><a class="reference internal" href="#general-notes">General Notes</a><ul>
<li><a class="reference internal" href="#folders-and-file-format">Folders and File Format</a></li>
<li><a class="reference internal" href="#surrogate-classifiers">Surrogate Classifiers</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gradient-descent-attack-notes">Gradient Descent Attack Notes</a><ul>
<li><a class="reference internal" href="#kde-distance">KDE distance</a></li>
<li><a class="reference internal" href="#folders-and-file-format-gradient-descent-attack">Folders and File Format</a></li>
</ul>
</li>
<li><a class="reference internal" href="#experimental-parameters">Experimental Parameters</a></li>
</ul>
</li>
<li><a class="reference internal" href="#developers-section">Developers Section</a><ul>
<li><a class="reference internal" href="#adding-new-attacks">Adding New Attacks</a></li>
<li><a class="reference internal" href="#adding-new-classifiers">Adding new classifiers</a></li>
</ul>
</li>
<li><a class="reference internal" href="#help-and-suggestions">Help and Suggestions</a></li>
<li><a class="reference internal" href="#contribute">Contribute</a></li>
<li><a class="reference internal" href="#snapshots">Snapshots</a></li>
<li><a class="reference internal" href="#bibliography">Bibliography</a></li>
</ul>
</li>
</ul>

  <h4>Next topic</h4>
  <p class="topless"><a href="matlab.html"
                        title="next chapter">Matlab Wrappers for AdversariaLib (with Examples)</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/advlib.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="matlab.html" title="Matlab Wrappers for AdversariaLib (with Examples)"
             >next</a> |</li>
        <li><a href="#">AdversariaLib 1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2014, Igino Corona, Battista Biggio, Davide Maiorca.
    </div>
  </body>
</html>